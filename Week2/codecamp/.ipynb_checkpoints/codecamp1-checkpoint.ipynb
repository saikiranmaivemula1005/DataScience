{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "req = requests.get(\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970\")\n",
    "# page = req.text\n",
    "\n",
    "# soup = BeautifulSoup(page,'html.parser')\n",
    "# print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'band_singer': 'The Guess Who', 'ranking': '3', 'title': '\"American Woman\"', 'url': '/wiki/American_Woman'}, {'band_singer': 'B.J. Thomas', 'ranking': '4', 'title': '\"Raindrops Keep Fallin\\' on My Head\"', 'url': '/wiki/Raindrops_Keep_Fallin%27_on_My_Head'}]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "page = req.text\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "soup.table[\"class\"]\n",
    "\n",
    "table_html = soup.find('table', attrs={'class':'wikitable sortable'})\n",
    "songs_list = []\n",
    "\n",
    "table_row_list = table_html.find_all('tr')\n",
    "table_rows = table_row_list[1:] \n",
    "table_rows\n",
    "\n",
    "for row in table_rows:\n",
    "#     print(row)\n",
    "    table_row_values = row.find_all('td')\n",
    "    \n",
    "#     print(table_cols)\n",
    "    \n",
    "    ranking = table_row_values[0].get_text()\n",
    "    title = table_row_values[1].get_text()\n",
    "    band_singer = table_row_values[2].get_text().replace(\"\\n\",\"\")\n",
    "    \n",
    "    url = table_row_values[1].a[\"href\"]\n",
    "    \n",
    "    dic = {'band_singer': band_singer,\n",
    "           'ranking': ranking,\n",
    "           'title': title,\n",
    "           'url': url}\n",
    "    songs_list.append(dic)\n",
    "print(songs_list[2:4])\n",
    "# my_table = soup.find('table',{'class':'wikitable sortable'}).findAll('tr')\n",
    "# songs = []\n",
    "# table = my_table[1:]\n",
    "# for row in table:\n",
    "#     dictionary = {}\n",
    "#     dictionary[\"ranking\"] = ''.join(row.find(\"td\").contents)\n",
    "#     anchor_tag = row.findAll(\"a\")\n",
    "#     temp = []\n",
    "#     for i in anchor_tag:\n",
    "#         temp.append(''.join(i.contents))\n",
    "#         temp.append(i['href'])\n",
    "#     if len(temp)!=0:\n",
    "#         dictionary['band_singer']  = temp[2]\n",
    "#         dictionary['title'] = temp[0]\n",
    "#         dictionary['url']=temp[3]\n",
    "#         songs.append(info)\n",
    "# print(songs[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = {}\n",
    "\n",
    "for y in range(1992,2015):\n",
    "    st = \"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\"+ str(y)\n",
    "    temp_req = requests.get(st)\n",
    "    years[y] = temp_req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_year(the_year, yeartext_dict):\n",
    "    year = the_year\n",
    "    yearinfo = []\n",
    "    song = []\n",
    "    songurl = []\n",
    "    band_singer = []\n",
    "    title = []\n",
    "    url = []\n",
    "    title_text = ''\n",
    "    i = 0\n",
    "    title_string = ''\n",
    "    band_singer = ''\n",
    "    soup = BeautifulSoup(years[year], \"lxml\")\n",
    "    tables = soup.find('table', attrs={'class':'wikitable sortable'})        \n",
    "    #iterates through tree structure, scraping our data\n",
    "    tr_list = tables.find_all('tr')    \n",
    "    for tr in tr_list:\n",
    "        td_list = tr.find_all('td')\n",
    "        if td_list == [] :\n",
    "            td_list = []\n",
    "        else : \n",
    "            ranking = tr.th.string\n",
    "            links = tr.td.findAll('a')\n",
    "            number_of_links = len(links)   \n",
    "            if number_of_links == 0:\n",
    "                songurl = [None]\n",
    "                title_text = [a['title']]\n",
    "                song.append(a['title'] )\n",
    "            else :\n",
    "                i = 0\n",
    "                for a in tr.td.findAll('a') : \n",
    "                    song.append(a['title'] )\n",
    "                    title_string = '\\\"' + a['title'] + '\\\"'    \n",
    "                    if i == 0 :\n",
    "                        title_text = title_string\n",
    "                        i = i + 1\n",
    "                    else :\n",
    "                        title_text = title_text + ' / ' + title_string\n",
    "                        i = i + 1    \n",
    "                    songurl.append(a['href'])\n",
    "            title = song\n",
    "            #finds next td tag\n",
    "            tr.td.findNext('td') \n",
    "            temp = len(tr.td.findNext('td').findAll('a'))\n",
    "            if temp == 0:\n",
    "                singer_url = [None]\n",
    "                band_singer = tr.td.findNext('td').string\n",
    "                band_singer = [band_singer]\n",
    "            elif temp == 1:\n",
    "                singer_url = tr.td.findNext('td').a['href']\n",
    "                singer_url = [singer_url]\n",
    "                band_singer = tr.td.findNext('td').a.string\n",
    "                band_singer = [band_singer]\n",
    "            else:\n",
    "                singer_url = []\n",
    "                band_singer = []\n",
    "                for a in tr.td.findNext('td').findAll('a'):\n",
    "                    webpage = a['href']\n",
    "                    singer_url.append(webpage)\n",
    "                    band_singer.append(a.string)            \n",
    "            #creates dictionary entry                   \n",
    "            dict_entry = {'band_singer' : band_singer,\n",
    "            'ranking' : ranking.replace(\"\\n\",\"\"),\n",
    "            'song' : title, 'songurl': songurl, 'titletext' : title_text,\n",
    "            'url' : singer_url}\n",
    "            #appends new dictionary to our list\n",
    "            yearinfo.append(dict_entry)      \n",
    "            songurl = []\n",
    "            song = []\n",
    "            title_string = ''\n",
    "            title_text = ''    \n",
    "    return(yearinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4bbfa4e46eb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0myears_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1992\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0myears_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparse_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1997\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-7178e825fd44>\u001b[0m in \u001b[0;36mparse_year\u001b[1;34m(the_year, yeartext_dict)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtitle_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mband_singer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'wikitable sortable'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#iterates through tree structure, scraping our data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                     % \",\".join(features))\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "years_info = {}\n",
    "for y in range(1992,2005):\n",
    "  years_info.update({y: parse_year(y,years)})\n",
    "  \n",
    "print(years_info[1997])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"yearinfo.json\", \"r\") as fd:\n",
    "    yearinfo = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for year in yearinfo.keys():\n",
    "    for song in yearinfo[year]:\n",
    "        song['year'] = year\n",
    "        rows.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_singer = []\n",
    "songurl = ''\n",
    "title_text = ''\n",
    "singer_url = []\n",
    "starting_length = len(rows)\n",
    "for dics in rows: \n",
    "    if starting_length == 0:\n",
    "        break\n",
    "    dict_add = {}   \n",
    "    # checks if our dictionary contains lists longer than one element\n",
    "    if len(dics['band_singer']) > 1:\n",
    "        i = 0\n",
    "        j = len(dics['band_singer'])\n",
    "        \n",
    "        for value in dics['band_singer']:\n",
    "            # new dictionary entry to append to our list\n",
    "            dict_add = {'band_singer' : dics['band_singer'][i],\n",
    "            'ranking' : dics['ranking'],\n",
    "            'song' : dics['song'], 'songurl': dics['songurl'], 'titletext' : dics['titletext'],\n",
    "            'url' : dics['url'][i], 'year' : dics['year']}\n",
    "            rows.append(dict_add)\n",
    "            i = 1 + i\n",
    "            j = j - 1\n",
    "    starting_length = starting_length - 1\n",
    "\n",
    "rows2 = []\n",
    "band_singer = []\n",
    "\n",
    "# loops through our list again, removing duplicate entries\n",
    "for dics in rows:\n",
    "    if len(dics['band_singer']) == 1 or len(dics['band_singer']) > 5:\n",
    "        rows2.append(dics)\n",
    "\n",
    "# turns all remaining one element lists into strings\n",
    "for row in rows2:\n",
    "    for key in row:\n",
    "        row[key] = str(row[key])\n",
    "        row[key] = row[key].strip(\"[]\")\n",
    "        row[key] = row[key].strip(\"''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows2\n",
    "flatframe = pd.DataFrame(rows2)\n",
    "\n",
    "\n",
    "# check datatypes of dataframe columns\n",
    "flatframe['year'].dtype\n",
    "flatframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_count = flatframe[\"band_singer\"].value_counts()\n",
    "\n",
    "artist_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlcache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # Check if URL has already been visited.\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(1)\n",
    "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "        try:\n",
    "            r = requests.get(\"http://en.wikipedia.org%s\" % url)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "    return urlcache[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatframe=flatframe.sort_values('year')\n",
    "flatframe.head()\n",
    "urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatframe[\"url\"].apply(get_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of bad requests:\",np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])) # no one or 0's)\n",
    "print(\"Did we get all urls?\", len(flatframe.url.unique())==len(urlcache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/artistinfo.json\",\"w\") as fd:\n",
    "    json.dump(urlcache, fd)\n",
    "del urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artistinfo.json\") as json_file:\n",
    "    urlcache = json.load(json_file)\n",
    "    \n",
    "urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singer_band_info(url, page_text):\n",
    "    bday_dict = {}\n",
    "    bday = ''\n",
    "    ya = ''\n",
    "    # soupify our webpage\n",
    "    soup = BeautifulSoup(page_text[url], \"lxml\")\n",
    "    tables = soup.find('table', attrs={'class':'infobox vcard plainlist'})\n",
    "    if (tables == None):\n",
    "        tables = soup.find('table', attrs={'class':'infobox biography vcard'})\n",
    "    bday = tables.find('span', {'class': 'bday'})\n",
    "    if bday: \n",
    "        bday = bday.get_text()[:4]\n",
    "        bday_dict = {'url' : url, 'born' : bday, 'ya' : ya}\n",
    "    else:\n",
    "        ya = False\n",
    "        for tr in tables.find_all('tr'):\n",
    "            if hasattr(tr.th, 'span'):\n",
    "                if hasattr(tr.th.span, 'string'):\n",
    "                    if tr.th.span.string == 'Years active':                \n",
    "                        if hasattr(tr.th, 'span'):\n",
    "                            #ya = tr.td.string\n",
    "                            ya = tr.td.text   #DK add\n",
    "                            bday = 'False'\n",
    "                            bday_dict = {'url' : url, 'born' : 'False', 'ya' : ya}\n",
    "    return(bday_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/wiki/Iggy_Azalea'\n",
    "singer_band_info(url, urlcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_addit_dicts = []\n",
    "bday_dict = {}\n",
    "for url in urlcache.keys():   \n",
    "    try:\n",
    "        bday_dict = singer_band_info(url, urlcache)\n",
    "        list_of_addit_dicts.append(bday_dict)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_df = pd.DataFrame(list_of_addit_dicts)\n",
    "\n",
    "largedf = pd.merge(flatframe, additional_df, left_on='url', right_on='url', how=\"outer\")\n",
    "largedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
